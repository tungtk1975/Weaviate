{"payload":{"allShortcutsEnabled":true,"fileTree":{"":{"items":[{"name":"api","path":"api","contentType":"directory"},{"name":"app","path":"app","contentType":"directory"},{"name":"cmd","path":"cmd","contentType":"directory"},{"name":"docs","path":"docs","contentType":"directory"},{"name":"examples","path":"examples","contentType":"directory"},{"name":"format","path":"format","contentType":"directory"},{"name":"llm","path":"llm","contentType":"directory"},{"name":"parser","path":"parser","contentType":"directory"},{"name":"progress","path":"progress","contentType":"directory"},{"name":"readline","path":"readline","contentType":"directory"},{"name":"scripts","path":"scripts","contentType":"directory"},{"name":"server","path":"server","contentType":"directory"},{"name":"version","path":"version","contentType":"directory"},{"name":".dockerignore","path":".dockerignore","contentType":"file"},{"name":".gitignore","path":".gitignore","contentType":"file"},{"name":".gitmodules","path":".gitmodules","contentType":"file"},{"name":".prettierrc.json","path":".prettierrc.json","contentType":"file"},{"name":"Dockerfile","path":"Dockerfile","contentType":"file"},{"name":"Dockerfile.build","path":"Dockerfile.build","contentType":"file"},{"name":"LICENSE","path":"LICENSE","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"go.mod","path":"go.mod","contentType":"file"},{"name":"go.sum","path":"go.sum","contentType":"file"},{"name":"main.go","path":"main.go","contentType":"file"}],"totalCount":24}},"fileTreeProcessingTime":5.063941,"foldersToFetch":[],"reducedMotionEnabled":"system","repo":{"id":658928958,"defaultBranch":"main","name":"ollama","ownerLogin":"jmorganca","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2023-06-27T02:39:32.000+07:00","ownerAvatar":"https://avatars.githubusercontent.com/u/251292?v=4","public":true,"private":false,"isOrgOwned":false},"symbolsExpanded":true,"treeExpanded":true,"refInfo":{"name":"main","listCacheKey":"v0:1703118445.0","canEdit":true,"refType":"branch","currentOid":"a607d922f0bd514e709b2392be9d5d5d2df3028a"},"path":"README.md","currentUser":{"id":113885894,"login":"tungtk1975","userEmail":"trankhanhtung@gmail.com"},"blob":{"rawLines":null,"stylingDirectives":null,"csv":null,"csvError":null,"dependabotInfo":{"showConfigurationBanner":false,"configFilePath":null,"networkDependabotPath":"/jmorganca/ollama/network/updates","dismissConfigurationNoticePath":"/settings/dismiss-notice/dependabot_configuration_notice","configurationNoticeDismissed":false,"repoAlertsPath":"/jmorganca/ollama/security/dependabot","repoSecurityAndAnalysisPath":"/jmorganca/ollama/settings/security_analysis","repoOwnerIsOrg":false,"currentUserCanAdminRepo":false},"displayName":"README.md","displayUrl":"https://github.com/jmorganca/ollama/blob/main/README.md?raw=true","headerInfo":{"blobSize":"9.11 KB","deleteInfo":{"deleteTooltip":"Fork this repository and delete the file"},"editInfo":{"editTooltip":"Fork this repository and edit the file"},"ghDesktopPath":"https://desktop.github.com","gitLfsPath":null,"onBranch":true,"shortPath":"8d1da60","siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2Fjmorganca%2Follama%2Fblob%2Fmain%2FREADME.md","isCSV":false,"isRichtext":true,"toc":[{"level":1,"text":"Ollama","anchor":"ollama","htmlText":"Ollama"},{"level":3,"text":"macOS","anchor":"macos","htmlText":"macOS"},{"level":3,"text":"Windows","anchor":"windows","htmlText":"Windows"},{"level":3,"text":"Linux & WSL2","anchor":"linux--wsl2","htmlText":"Linux &amp; WSL2"},{"level":3,"text":"Docker","anchor":"docker","htmlText":"Docker"},{"level":2,"text":"Quickstart","anchor":"quickstart","htmlText":"Quickstart"},{"level":2,"text":"Model library","anchor":"model-library","htmlText":"Model library"},{"level":2,"text":"Customize your own model","anchor":"customize-your-own-model","htmlText":"Customize your own model"},{"level":3,"text":"Import from GGUF","anchor":"import-from-gguf","htmlText":"Import from GGUF"},{"level":3,"text":"Import from PyTorch or Safetensors","anchor":"import-from-pytorch-or-safetensors","htmlText":"Import from PyTorch or Safetensors"},{"level":3,"text":"Customize a prompt","anchor":"customize-a-prompt","htmlText":"Customize a prompt"},{"level":2,"text":"CLI Reference","anchor":"cli-reference","htmlText":"CLI Reference"},{"level":3,"text":"Create a model","anchor":"create-a-model","htmlText":"Create a model"},{"level":3,"text":"Pull a model","anchor":"pull-a-model","htmlText":"Pull a model"},{"level":3,"text":"Remove a model","anchor":"remove-a-model","htmlText":"Remove a model"},{"level":3,"text":"Copy a model","anchor":"copy-a-model","htmlText":"Copy a model"},{"level":3,"text":"Multiline input","anchor":"multiline-input","htmlText":"Multiline input"},{"level":3,"text":"Multimodal models","anchor":"multimodal-models","htmlText":"Multimodal models"},{"level":3,"text":"Pass in prompt as arguments","anchor":"pass-in-prompt-as-arguments","htmlText":"Pass in prompt as arguments"},{"level":3,"text":"List models on your computer","anchor":"list-models-on-your-computer","htmlText":"List models on your computer"},{"level":3,"text":"Start Ollama","anchor":"start-ollama","htmlText":"Start Ollama"},{"level":2,"text":"Building","anchor":"building","htmlText":"Building"},{"level":2,"text":"REST API","anchor":"rest-api","htmlText":"REST API"},{"level":3,"text":"Generate a response","anchor":"generate-a-response","htmlText":"Generate a response"},{"level":3,"text":"Chat with a model","anchor":"chat-with-a-model","htmlText":"Chat with a model"},{"level":2,"text":"Community Integrations","anchor":"community-integrations","htmlText":"Community Integrations"},{"level":3,"text":"Web & Desktop","anchor":"web--desktop","htmlText":"Web &amp; Desktop"},{"level":3,"text":"Terminal","anchor":"terminal","htmlText":"Terminal"},{"level":3,"text":"Database","anchor":"database","htmlText":"Database"},{"level":3,"text":"Package managers","anchor":"package-managers","htmlText":"Package managers"},{"level":3,"text":"Libraries","anchor":"libraries","htmlText":"Libraries"},{"level":3,"text":"Mobile","anchor":"mobile","htmlText":"Mobile"},{"level":3,"text":"Extensions & Plugins","anchor":"extensions--plugins","htmlText":"Extensions &amp; Plugins"}],"lineInfo":{"truncatedLoc":"308","truncatedSloc":"216"},"mode":"file"},"image":false,"isCodeownersFile":null,"isPlain":false,"isValidLegacyIssueTemplate":false,"issueTemplateHelpUrl":"https://docs.github.com/articles/about-issue-and-pull-request-templates","issueTemplate":null,"discussionTemplate":null,"language":"Markdown","languageID":222,"large":false,"loggedIn":true,"newDiscussionPath":"/jmorganca/ollama/discussions/new","newIssuePath":"/jmorganca/ollama/issues/new","planSupportInfo":{"repoIsFork":null,"repoOwnedByCurrentUser":null,"requestFullPath":"/jmorganca/ollama/blob/main/README.md","showFreeOrgGatedFeatureMessage":null,"showPlanSupportBanner":null,"upgradeDataAttributes":null,"upgradePath":null},"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_dockerfile","dismissStackNoticePath":"/settings/dismiss-notice/publish_stack_from_file","releasePath":"/jmorganca/ollama/releases/new?marketplace=true","showPublishActionBanner":false,"showPublishStackBanner":false},"rawBlobUrl":"https://github.com/jmorganca/ollama/raw/main/README.md","renderImageOrRaw":false,"richText":"<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div align=\"center\" dir=\"auto\">\n  <themed-picture data-catalyst-inline=\"true\"><picture>\n    <source media=\"(prefers-color-scheme: dark)\" height=\"200px\" srcset=\"https://github.com/jmorganca/ollama/assets/3325447/56ea1849-1284-4645-8970-956de6e51c3c\">\n    <img alt=\"logo\" height=\"200px\" src=\"https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n  </picture></themed-picture>\n</div>\n<h1 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-ollama\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#ollama\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Ollama</h1>\n<p dir=\"auto\"><a href=\"https://discord.gg/ollama\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ef368e35e49b20967e9a50d4d14f2eaa8beee7f7f608675f5ae21edd84e462e3/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f6f6c6c616d613f7374796c653d666c617426636f6d706163743d74727565\" alt=\"Discord\" data-canonical-src=\"https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;compact=true\" style=\"max-width: 100%;\"></a></p>\n<p dir=\"auto\">Get up and running with large language models locally.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-macos\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#macos\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>macOS</h3>\n<p dir=\"auto\"><a href=\"https://ollama.ai/download/Ollama-darwin.zip\" rel=\"nofollow\">Download</a></p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-windows\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#windows\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Windows</h3>\n<p dir=\"auto\">Coming soon! For now, you can install Ollama on Windows via WSL2.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-linux--wsl2\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#linux--wsl2\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Linux &amp; WSL2</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"curl https://ollama.ai/install.sh | sh\"><pre class=\"notranslate\"><code>curl https://ollama.ai/install.sh | sh\n</code></pre></div>\n<p dir=\"auto\"><a href=\"https://github.com/jmorganca/ollama/blob/main/docs/linux.md\">Manual install instructions</a></p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-docker\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#docker\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Docker</h3>\n<p dir=\"auto\">The official <a href=\"https://hub.docker.com/r/ollama/ollama\" rel=\"nofollow\">Ollama Docker image</a> <code>ollama/ollama</code> is available on Docker Hub.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-quickstart\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#quickstart\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Quickstart</h2>\n<p dir=\"auto\">To run and chat with <a href=\"https://ollama.ai/library/llama2\" rel=\"nofollow\">Llama 2</a>:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama run llama2\"><pre class=\"notranslate\"><code>ollama run llama2\n</code></pre></div>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-model-library\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#model-library\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Model library</h2>\n<p dir=\"auto\">Ollama supports a list of open-source models available on <a href=\"https://ollama.ai/library\" title=\"ollama model library\" rel=\"nofollow\">ollama.ai/library</a></p>\n<p dir=\"auto\">Here are some example open-source models that can be downloaded:</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Parameters</th>\n<th>Size</th>\n<th>Download</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Llama 2</td>\n<td>7B</td>\n<td>3.8GB</td>\n<td><code>ollama run llama2</code></td>\n</tr>\n<tr>\n<td>Mistral</td>\n<td>7B</td>\n<td>4.1GB</td>\n<td><code>ollama run mistral</code></td>\n</tr>\n<tr>\n<td>Phi-2</td>\n<td>2.7B</td>\n<td>1.7GB</td>\n<td><code>ollama run phi</code></td>\n</tr>\n<tr>\n<td>Neural Chat</td>\n<td>7B</td>\n<td>4.1GB</td>\n<td><code>ollama run neural-chat</code></td>\n</tr>\n<tr>\n<td>Starling</td>\n<td>7B</td>\n<td>4.1GB</td>\n<td><code>ollama run starling-lm</code></td>\n</tr>\n<tr>\n<td>Code Llama</td>\n<td>7B</td>\n<td>3.8GB</td>\n<td><code>ollama run codellama</code></td>\n</tr>\n<tr>\n<td>Llama 2 Uncensored</td>\n<td>7B</td>\n<td>3.8GB</td>\n<td><code>ollama run llama2-uncensored</code></td>\n</tr>\n<tr>\n<td>Llama 2 13B</td>\n<td>13B</td>\n<td>7.3GB</td>\n<td><code>ollama run llama2:13b</code></td>\n</tr>\n<tr>\n<td>Llama 2 70B</td>\n<td>70B</td>\n<td>39GB</td>\n<td><code>ollama run llama2:70b</code></td>\n</tr>\n<tr>\n<td>Orca Mini</td>\n<td>3B</td>\n<td>1.9GB</td>\n<td><code>ollama run orca-mini</code></td>\n</tr>\n<tr>\n<td>Vicuna</td>\n<td>7B</td>\n<td>3.8GB</td>\n<td><code>ollama run vicuna</code></td>\n</tr>\n<tr>\n<td>LLaVA</td>\n<td>7B</td>\n<td>4.5GB</td>\n<td><code>ollama run llava</code></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p dir=\"auto\">Note: You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.</p>\n</blockquote>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-customize-your-own-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#customize-your-own-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Customize your own model</h2>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-import-from-gguf\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#import-from-gguf\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Import from GGUF</h3>\n<p dir=\"auto\">Ollama supports importing GGUF models in the Modelfile:</p>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\">Create a file named <code>Modelfile</code>, with a <code>FROM</code> instruction with the local filepath to the model you want to import.</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"FROM ./vicuna-33b.Q4_0.gguf\"><pre class=\"notranslate\"><code>FROM ./vicuna-33b.Q4_0.gguf\n</code></pre></div>\n</li>\n<li>\n<p dir=\"auto\">Create the model in Ollama</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama create example -f Modelfile\"><pre class=\"notranslate\"><code>ollama create example -f Modelfile\n</code></pre></div>\n</li>\n<li>\n<p dir=\"auto\">Run the model</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama run example\"><pre class=\"notranslate\"><code>ollama run example\n</code></pre></div>\n</li>\n</ol>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-import-from-pytorch-or-safetensors\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#import-from-pytorch-or-safetensors\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Import from PyTorch or Safetensors</h3>\n<p dir=\"auto\">See the <a href=\"/jmorganca/ollama/blob/main/docs/import.md\">guide</a> on importing models for more information.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-customize-a-prompt\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#customize-a-prompt\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Customize a prompt</h3>\n<p dir=\"auto\">Models from the Ollama library can be customized with a prompt. For example, to customize the <code>llama2</code> model:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama pull llama2\"><pre class=\"notranslate\"><code>ollama pull llama2\n</code></pre></div>\n<p dir=\"auto\">Create a <code>Modelfile</code>:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"FROM llama2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM &quot;&quot;&quot;\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n&quot;&quot;&quot;\"><pre class=\"notranslate\"><code>FROM llama2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n</code></pre></div>\n<p dir=\"auto\">Next, create and run the model:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama create mario -f ./Modelfile\nollama run mario\n&gt;&gt;&gt; hi\nHello! It's your friend Mario.\"><pre class=\"notranslate\"><code>ollama create mario -f ./Modelfile\nollama run mario\n&gt;&gt;&gt; hi\nHello! It's your friend Mario.\n</code></pre></div>\n<p dir=\"auto\">For more examples, see the <a href=\"/jmorganca/ollama/blob/main/examples\">examples</a> directory. For more information on working with a Modelfile, see the <a href=\"/jmorganca/ollama/blob/main/docs/modelfile.md\">Modelfile</a> documentation.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-cli-reference\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#cli-reference\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>CLI Reference</h2>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-create-a-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#create-a-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Create a model</h3>\n<p dir=\"auto\"><code>ollama create</code> is used to create a model from a Modelfile.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-pull-a-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#pull-a-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Pull a model</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama pull llama2\"><pre class=\"notranslate\"><code>ollama pull llama2\n</code></pre></div>\n<blockquote>\n<p dir=\"auto\">This command can also be used to update a local model. Only the diff will be pulled.</p>\n</blockquote>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-remove-a-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#remove-a-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Remove a model</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama rm llama2\"><pre class=\"notranslate\"><code>ollama rm llama2\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-copy-a-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#copy-a-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Copy a model</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama cp llama2 my-llama2\"><pre class=\"notranslate\"><code>ollama cp llama2 my-llama2\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-multiline-input\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#multiline-input\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Multiline input</h3>\n<p dir=\"auto\">For multiline input, you can wrap text with <code>\"\"\"</code>:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&gt;&gt;&gt; &quot;&quot;&quot;Hello,\n... world!\n... &quot;&quot;&quot;\nI'm a basic program that prints the famous &quot;Hello, world!&quot; message to the console.\"><pre class=\"notranslate\"><code>&gt;&gt;&gt; \"\"\"Hello,\n... world!\n... \"\"\"\nI'm a basic program that prints the famous \"Hello, world!\" message to the console.\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-multimodal-models\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#multimodal-models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Multimodal models</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&gt;&gt;&gt; What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\"><pre class=\"notranslate\"><code>&gt;&gt;&gt; What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-pass-in-prompt-as-arguments\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#pass-in-prompt-as-arguments\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Pass in prompt as arguments</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"$ ollama run llama2 &quot;Summarize this file: $(cat README.md)&quot;\n Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\"><pre class=\"notranslate\"><code>$ ollama run llama2 \"Summarize this file: $(cat README.md)\"\n Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-list-models-on-your-computer\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#list-models-on-your-computer\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>List models on your computer</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ollama list\"><pre class=\"notranslate\"><code>ollama list\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-start-ollama\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#start-ollama\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Start Ollama</h3>\n<p dir=\"auto\"><code>ollama serve</code> is used when you want to start ollama without running the desktop application.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#building\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Building</h2>\n<p dir=\"auto\">Install <code>cmake</code> and <code>go</code>:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"brew install cmake go\"><pre class=\"notranslate\"><code>brew install cmake go\n</code></pre></div>\n<p dir=\"auto\">Then generate dependencies and build:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"go generate ./...\ngo build .\"><pre class=\"notranslate\"><code>go generate ./...\ngo build .\n</code></pre></div>\n<p dir=\"auto\">Next, start the server:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"./ollama serve\"><pre class=\"notranslate\"><code>./ollama serve\n</code></pre></div>\n<p dir=\"auto\">Finally, in a separate shell, run a model:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"./ollama run llama2\"><pre class=\"notranslate\"><code>./ollama run llama2\n</code></pre></div>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-rest-api\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#rest-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>REST API</h2>\n<p dir=\"auto\">Ollama has a REST API for running and managing models.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-generate-a-response\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#generate-a-response\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Generate a response</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama2&quot;,\n  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;\n}'\"><pre class=\"notranslate\"><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n</code></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-chat-with-a-model\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#chat-with-a-model\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Chat with a model</h3>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;mistral&quot;,\n  &quot;messages&quot;: [\n    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; }\n  ]\n}'\"><pre class=\"notranslate\"><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"mistral\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n</code></pre></div>\n<p dir=\"auto\">See the <a href=\"/jmorganca/ollama/blob/main/docs/api.md\">API documentation</a> for all endpoints.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-community-integrations\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#community-integrations\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Community Integrations</h2>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-web--desktop\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#web--desktop\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Web &amp; Desktop</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/bionic-gpt/bionic-gpt\">Bionic GPT</a></li>\n<li><a href=\"https://github.com/rtcfirefly/ollama-ui\">HTML UI</a></li>\n<li><a href=\"https://github.com/ivanfioravanti/chatbot-ollama\">Chatbot UI</a></li>\n<li><a href=\"https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file\">Typescript UI</a></li>\n<li><a href=\"https://github.com/richawo/minimal-llm-ui\">Minimalistic React UI for Ollama Models</a></li>\n<li><a href=\"https://github.com/ollama-webui/ollama-webui\">Web UI</a></li>\n<li><a href=\"https://github.com/kevinhermawan/Ollamac\">Ollamac</a></li>\n<li><a href=\"https://github.com/enricoros/big-agi/blob/main/docs/config-ollama.md\">big-AGI</a></li>\n<li><a href=\"https://github.com/cheshire-cat-ai/core\">Cheshire Cat assistant framework</a></li>\n<li><a href=\"https://github.com/semperai/amica\">Amica</a></li>\n<li><a href=\"https://github.com/BruceMacD/chatd\">chatd</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-terminal\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#terminal\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Terminal</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/ggozad/oterm\">oterm</a></li>\n<li><a href=\"https://github.com/s-kostyaev/ellama\">Ellama Emacs client</a></li>\n<li><a href=\"https://github.com/zweifisch/ollama\">Emacs client</a></li>\n<li><a href=\"https://github.com/David-Kunz/gen.nvim\">gen.nvim</a></li>\n<li><a href=\"https://github.com/nomnivore/ollama.nvim\">ollama.nvim</a></li>\n<li><a href=\"https://github.com/huynle/ogpt.nvim\">ogpt.nvim</a></li>\n<li><a href=\"https://github.com/karthink/gptel\">gptel Emacs client</a></li>\n<li><a href=\"https://github.com/dustinblackman/oatmeal\">Oatmeal</a></li>\n<li><a href=\"https://github.com/pgibler/cmdh\">cmdh</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-database\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#database\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Database</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/ollama_handler/README.md\">MindsDB</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-package-managers\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#package-managers\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Package managers</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://archlinux.org/packages/extra/x86_64/ollama/\" rel=\"nofollow\">Pacman</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-libraries\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#libraries\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Libraries</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://python.langchain.com/docs/integrations/llms/ollama\" rel=\"nofollow\">LangChain</a> and <a href=\"https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama\" rel=\"nofollow\">LangChain.js</a> with <a href=\"https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\" rel=\"nofollow\">example</a></li>\n<li><a href=\"https://github.com/tmc/langchaingo/\">LangChainGo</a> with <a href=\"https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example\">example</a></li>\n<li><a href=\"https://gpt-index.readthedocs.io/en/stable/examples/llm/ollama.html\" rel=\"nofollow\">LlamaIndex</a></li>\n<li><a href=\"https://github.com/BerriAI/litellm\">LiteLLM</a></li>\n<li><a href=\"https://github.com/awaescher/OllamaSharp\">OllamaSharp for .NET</a></li>\n<li><a href=\"https://github.com/pepperoni21/ollama-rs\">Ollama-rs for Rust</a></li>\n<li><a href=\"https://github.com/amithkoujalgi/ollama4j\">Ollama4j for Java</a></li>\n<li><a href=\"https://modelfusion.dev/integration/model-provider/ollama\" rel=\"nofollow\">ModelFusion Typescript Library</a></li>\n<li><a href=\"https://github.com/kevinhermawan/OllamaKit\">OllamaKit for Swift</a></li>\n<li><a href=\"https://github.com/breitburg/dart-ollama\">Ollama for Dart</a></li>\n<li><a href=\"https://github.com/cloudstudio/ollama-laravel\">Ollama for Laravel</a></li>\n<li><a href=\"https://github.com/davidmigloz/langchain_dart\">LangChainDart</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-mobile\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#mobile\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Mobile</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/AugustDev/enchanted\">Enchanted</a></li>\n<li><a href=\"https://github.com/danemadsen/Maid\">Maid</a></li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-extensions--plugins\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#extensions--plugins\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Extensions &amp; Plugins</h3>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/MassimilianoPasquini97/raycast_ollama\">Raycast extension</a></li>\n<li><a href=\"https://github.com/mxyng/discollama\">Discollama</a> (Discord bot inside the Ollama discord channel)</li>\n<li><a href=\"https://github.com/continuedev/continue\">Continue</a></li>\n<li><a href=\"https://github.com/hinterdupfinger/obsidian-ollama\">Obsidian Ollama plugin</a></li>\n<li><a href=\"https://github.com/omagdy7/ollama-logseq\">Logseq Ollama plugin</a></li>\n<li><a href=\"https://github.com/samalba/dagger-chatbot\">Dagger Chatbot</a></li>\n<li><a href=\"https://github.com/mekb-turtle/discord-ai-bot\">Discord AI Bot</a></li>\n<li><a href=\"https://github.com/ruecat/ollama-telegram\">Ollama Telegram Bot</a></li>\n<li><a href=\"https://github.com/ej52/hass-ollama-conversation\">Hass Ollama Conversation</a></li>\n<li><a href=\"https://github.com/abrenneke/rivet-plugin-ollama\">Rivet plugin</a></li>\n<li><a href=\"https://github.com/ex3ndr/llama-coder\">Llama Coder</a> (Copilot alternative using Ollama)</li>\n<li><a href=\"https://github.com/longy2k/obsidian-bmo-chatbot\">Obsidian BMO Chatbot plugin</a></li>\n</ul>\n</article>","renderedFileInfo":null,"shortPath":null,"tabSize":8,"topBannersInfo":{"overridingGlobalFundingFile":false,"globalPreferredFundingPath":null,"repoOwner":"jmorganca","repoName":"ollama","showInvalidCitationWarning":false,"citationHelpUrl":"https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files","showDependabotConfigurationBanner":false,"actionsOnboardingTip":null},"truncated":false,"viewable":true,"workflowRedirectUrl":null,"symbols":{"timed_out":false,"not_analyzed":false,"symbols":[{"name":"Ollama","kind":"section_1","ident_start":348,"ident_end":354,"extent_start":346,"extent_end":9331,"fully_qualified_name":"Ollama","ident_utf16":{"start":{"line_number":7,"utf16_col":2},"end":{"line_number":7,"utf16_col":8}},"extent_utf16":{"start":{"line_number":7,"utf16_col":0},"end":{"line_number":308,"utf16_col":0}}},{"name":"macOS","kind":"section_3","ident_start":527,"ident_end":532,"extent_start":523,"extent_end":592,"fully_qualified_name":"macOS","ident_utf16":{"start":{"line_number":13,"utf16_col":4},"end":{"line_number":13,"utf16_col":9}},"extent_utf16":{"start":{"line_number":13,"utf16_col":0},"end":{"line_number":17,"utf16_col":0}}},{"name":"Windows","kind":"section_3","ident_start":596,"ident_end":603,"extent_start":592,"extent_end":672,"fully_qualified_name":"Windows","ident_utf16":{"start":{"line_number":17,"utf16_col":4},"end":{"line_number":17,"utf16_col":11}},"extent_utf16":{"start":{"line_number":17,"utf16_col":0},"end":{"line_number":21,"utf16_col":0}}},{"name":"Linux & WSL2","kind":"section_3","ident_start":676,"ident_end":688,"extent_start":672,"extent_end":830,"fully_qualified_name":"Linux & WSL2","ident_utf16":{"start":{"line_number":21,"utf16_col":4},"end":{"line_number":21,"utf16_col":16}},"extent_utf16":{"start":{"line_number":21,"utf16_col":0},"end":{"line_number":29,"utf16_col":0}}},{"name":"Docker","kind":"section_3","ident_start":834,"ident_end":840,"extent_start":830,"extent_end":962,"fully_qualified_name":"Docker","ident_utf16":{"start":{"line_number":29,"utf16_col":4},"end":{"line_number":29,"utf16_col":10}},"extent_utf16":{"start":{"line_number":29,"utf16_col":0},"end":{"line_number":33,"utf16_col":0}}},{"name":"Quickstart","kind":"section_2","ident_start":965,"ident_end":975,"extent_start":962,"extent_end":1071,"fully_qualified_name":"Quickstart","ident_utf16":{"start":{"line_number":33,"utf16_col":3},"end":{"line_number":33,"utf16_col":13}},"extent_utf16":{"start":{"line_number":33,"utf16_col":0},"end":{"line_number":41,"utf16_col":0}}},{"name":"Model library","kind":"section_2","ident_start":1074,"ident_end":1087,"extent_start":1071,"extent_end":2492,"fully_qualified_name":"Model library","ident_utf16":{"start":{"line_number":41,"utf16_col":3},"end":{"line_number":41,"utf16_col":16}},"extent_utf16":{"start":{"line_number":41,"utf16_col":0},"end":{"line_number":64,"utf16_col":0}}},{"name":"Customize your own model","kind":"section_2","ident_start":2495,"ident_end":2519,"extent_start":2492,"extent_end":3741,"fully_qualified_name":"Customize your own model","ident_utf16":{"start":{"line_number":64,"utf16_col":3},"end":{"line_number":64,"utf16_col":27}},"extent_utf16":{"start":{"line_number":64,"utf16_col":0},"end":{"line_number":125,"utf16_col":0}}},{"name":"Import from GGUF","kind":"section_3","ident_start":2525,"ident_end":2541,"extent_start":2521,"extent_end":2905,"fully_qualified_name":"Import from GGUF","ident_utf16":{"start":{"line_number":66,"utf16_col":4},"end":{"line_number":66,"utf16_col":20}},"extent_utf16":{"start":{"line_number":66,"utf16_col":0},"end":{"line_number":88,"utf16_col":0}}},{"name":"Import from PyTorch or Safetensors","kind":"section_3","ident_start":2909,"ident_end":2943,"extent_start":2905,"extent_end":3020,"fully_qualified_name":"Import from PyTorch or Safetensors","ident_utf16":{"start":{"line_number":88,"utf16_col":4},"end":{"line_number":88,"utf16_col":38}},"extent_utf16":{"start":{"line_number":88,"utf16_col":0},"end":{"line_number":92,"utf16_col":0}}},{"name":"Customize a prompt","kind":"section_3","ident_start":3024,"ident_end":3042,"extent_start":3020,"extent_end":3741,"fully_qualified_name":"Customize a prompt","ident_utf16":{"start":{"line_number":92,"utf16_col":4},"end":{"line_number":92,"utf16_col":22}},"extent_utf16":{"start":{"line_number":92,"utf16_col":0},"end":{"line_number":125,"utf16_col":0}}},{"name":"CLI Reference","kind":"section_2","ident_start":3744,"ident_end":3757,"extent_start":3741,"extent_end":4998,"fully_qualified_name":"CLI Reference","ident_utf16":{"start":{"line_number":125,"utf16_col":3},"end":{"line_number":125,"utf16_col":16}},"extent_utf16":{"start":{"line_number":125,"utf16_col":0},"end":{"line_number":186,"utf16_col":0}}},{"name":"Create a model","kind":"section_3","ident_start":3763,"ident_end":3777,"extent_start":3759,"extent_end":3840,"fully_qualified_name":"Create a model","ident_utf16":{"start":{"line_number":127,"utf16_col":4},"end":{"line_number":127,"utf16_col":18}},"extent_utf16":{"start":{"line_number":127,"utf16_col":0},"end":{"line_number":131,"utf16_col":0}}},{"name":"Pull a model","kind":"section_3","ident_start":3844,"ident_end":3856,"extent_start":3840,"extent_end":3974,"fully_qualified_name":"Pull a model","ident_utf16":{"start":{"line_number":131,"utf16_col":4},"end":{"line_number":131,"utf16_col":16}},"extent_utf16":{"start":{"line_number":131,"utf16_col":0},"end":{"line_number":139,"utf16_col":0}}},{"name":"Remove a model","kind":"section_3","ident_start":3978,"ident_end":3992,"extent_start":3974,"extent_end":4020,"fully_qualified_name":"Remove a model","ident_utf16":{"start":{"line_number":139,"utf16_col":4},"end":{"line_number":139,"utf16_col":18}},"extent_utf16":{"start":{"line_number":139,"utf16_col":0},"end":{"line_number":145,"utf16_col":0}}},{"name":"Copy a model","kind":"section_3","ident_start":4024,"ident_end":4036,"extent_start":4020,"extent_end":4074,"fully_qualified_name":"Copy a model","ident_utf16":{"start":{"line_number":145,"utf16_col":4},"end":{"line_number":145,"utf16_col":16}},"extent_utf16":{"start":{"line_number":145,"utf16_col":0},"end":{"line_number":151,"utf16_col":0}}},{"name":"Multiline input","kind":"section_3","ident_start":4078,"ident_end":4093,"extent_start":4074,"extent_end":4272,"fully_qualified_name":"Multiline input","ident_utf16":{"start":{"line_number":151,"utf16_col":4},"end":{"line_number":151,"utf16_col":19}},"extent_utf16":{"start":{"line_number":151,"utf16_col":0},"end":{"line_number":162,"utf16_col":0}}},{"name":"Multimodal models","kind":"section_3","ident_start":4276,"ident_end":4293,"extent_start":4272,"extent_end":4454,"fully_qualified_name":"Multimodal models","ident_utf16":{"start":{"line_number":162,"utf16_col":4},"end":{"line_number":162,"utf16_col":21}},"extent_utf16":{"start":{"line_number":162,"utf16_col":0},"end":{"line_number":169,"utf16_col":0}}},{"name":"Pass in prompt as arguments","kind":"section_3","ident_start":4458,"ident_end":4485,"extent_start":4454,"extent_end":4830,"fully_qualified_name":"Pass in prompt as arguments","ident_utf16":{"start":{"line_number":169,"utf16_col":4},"end":{"line_number":169,"utf16_col":31}},"extent_utf16":{"start":{"line_number":169,"utf16_col":0},"end":{"line_number":176,"utf16_col":0}}},{"name":"List models on your computer","kind":"section_3","ident_start":4834,"ident_end":4862,"extent_start":4830,"extent_end":4885,"fully_qualified_name":"List models on your computer","ident_utf16":{"start":{"line_number":176,"utf16_col":4},"end":{"line_number":176,"utf16_col":32}},"extent_utf16":{"start":{"line_number":176,"utf16_col":0},"end":{"line_number":182,"utf16_col":0}}},{"name":"Start Ollama","kind":"section_3","ident_start":4889,"ident_end":4901,"extent_start":4885,"extent_end":4998,"fully_qualified_name":"Start Ollama","ident_utf16":{"start":{"line_number":182,"utf16_col":4},"end":{"line_number":182,"utf16_col":16}},"extent_utf16":{"start":{"line_number":182,"utf16_col":0},"end":{"line_number":186,"utf16_col":0}}},{"name":"Building","kind":"section_2","ident_start":5001,"ident_end":5009,"extent_start":4998,"extent_end":5268,"fully_qualified_name":"Building","ident_utf16":{"start":{"line_number":186,"utf16_col":3},"end":{"line_number":186,"utf16_col":11}},"extent_utf16":{"start":{"line_number":186,"utf16_col":0},"end":{"line_number":213,"utf16_col":0}}},{"name":"REST API","kind":"section_2","ident_start":5271,"ident_end":5279,"extent_start":5268,"extent_end":5717,"fully_qualified_name":"REST API","ident_utf16":{"start":{"line_number":213,"utf16_col":3},"end":{"line_number":213,"utf16_col":11}},"extent_utf16":{"start":{"line_number":213,"utf16_col":0},"end":{"line_number":239,"utf16_col":0}}},{"name":"Generate a response","kind":"section_3","ident_start":5341,"ident_end":5360,"extent_start":5337,"extent_end":5476,"fully_qualified_name":"Generate a response","ident_utf16":{"start":{"line_number":217,"utf16_col":4},"end":{"line_number":217,"utf16_col":23}},"extent_utf16":{"start":{"line_number":217,"utf16_col":0},"end":{"line_number":226,"utf16_col":0}}},{"name":"Chat with a model","kind":"section_3","ident_start":5480,"ident_end":5497,"extent_start":5476,"extent_end":5717,"fully_qualified_name":"Chat with a model","ident_utf16":{"start":{"line_number":226,"utf16_col":4},"end":{"line_number":226,"utf16_col":21}},"extent_utf16":{"start":{"line_number":226,"utf16_col":0},"end":{"line_number":239,"utf16_col":0}}},{"name":"Community Integrations","kind":"section_2","ident_start":5720,"ident_end":5742,"extent_start":5717,"extent_end":9331,"fully_qualified_name":"Community Integrations","ident_utf16":{"start":{"line_number":239,"utf16_col":3},"end":{"line_number":239,"utf16_col":25}},"extent_utf16":{"start":{"line_number":239,"utf16_col":0},"end":{"line_number":308,"utf16_col":0}}},{"name":"Web & Desktop","kind":"section_3","ident_start":5748,"ident_end":5761,"extent_start":5744,"extent_end":6472,"fully_qualified_name":"Web & Desktop","ident_utf16":{"start":{"line_number":241,"utf16_col":4},"end":{"line_number":241,"utf16_col":17}},"extent_utf16":{"start":{"line_number":241,"utf16_col":0},"end":{"line_number":254,"utf16_col":0}}},{"name":"Terminal","kind":"section_3","ident_start":6476,"ident_end":6484,"extent_start":6472,"extent_end":6963,"fully_qualified_name":"Terminal","ident_utf16":{"start":{"line_number":254,"utf16_col":4},"end":{"line_number":254,"utf16_col":12}},"extent_utf16":{"start":{"line_number":254,"utf16_col":0},"end":{"line_number":266,"utf16_col":0}}},{"name":"Database","kind":"section_3","ident_start":6967,"ident_end":6975,"extent_start":6963,"extent_end":7094,"fully_qualified_name":"Database","ident_utf16":{"start":{"line_number":266,"utf16_col":4},"end":{"line_number":266,"utf16_col":12}},"extent_utf16":{"start":{"line_number":266,"utf16_col":0},"end":{"line_number":270,"utf16_col":0}}},{"name":"Package managers","kind":"section_3","ident_start":7098,"ident_end":7114,"extent_start":7094,"extent_end":7181,"fully_qualified_name":"Package managers","ident_utf16":{"start":{"line_number":270,"utf16_col":4},"end":{"line_number":270,"utf16_col":20}},"extent_utf16":{"start":{"line_number":270,"utf16_col":0},"end":{"line_number":274,"utf16_col":0}}},{"name":"Libraries","kind":"section_3","ident_start":7185,"ident_end":7194,"extent_start":7181,"extent_end":8301,"fully_qualified_name":"Libraries","ident_utf16":{"start":{"line_number":274,"utf16_col":4},"end":{"line_number":274,"utf16_col":13}},"extent_utf16":{"start":{"line_number":274,"utf16_col":0},"end":{"line_number":289,"utf16_col":0}}},{"name":"Mobile","kind":"section_3","ident_start":8305,"ident_end":8311,"extent_start":8301,"extent_end":8413,"fully_qualified_name":"Mobile","ident_utf16":{"start":{"line_number":289,"utf16_col":4},"end":{"line_number":289,"utf16_col":10}},"extent_utf16":{"start":{"line_number":289,"utf16_col":0},"end":{"line_number":294,"utf16_col":0}}},{"name":"Extensions & Plugins","kind":"section_3","ident_start":8417,"ident_end":8437,"extent_start":8413,"extent_end":9331,"fully_qualified_name":"Extensions & Plugins","ident_utf16":{"start":{"line_number":294,"utf16_col":4},"end":{"line_number":294,"utf16_col":24}},"extent_utf16":{"start":{"line_number":294,"utf16_col":0},"end":{"line_number":308,"utf16_col":0}}}]}},"copilotInfo":{"documentationUrl":"https://docs.github.com/copilot/overview-of-github-copilot/about-github-copilot-for-business","notices":{"codeViewPopover":{"dismissed":false,"dismissPath":"/settings/dismiss-notice/code_view_copilot_popover"}},"userAccess":{"accessAllowed":false,"hasSubscriptionEnded":false,"orgHasCFBAccess":false,"userHasCFIAccess":false,"userHasOrgs":true,"userIsOrgAdmin":false,"userIsOrgMember":false,"business":null,"featureRequestInfo":null}},"copilotAccessAllowed":false,"csrf_tokens":{"/jmorganca/ollama/branches":{"post":"Bsl3PUMYr0s9w8kqoYEkz-mF6jV59_B3Ib7cEzRuU-YBAUbvN7qu9h3STHnBEg1PHuaZJtusIoDFnaQEantdzw"},"/repos/preferences":{"post":"svKQYhzCiRL--jrG6x7ep59rtTYFSOp8dDuVPqgt8b8Qw5-81x6LWg52Jv-Ii49o7d70XafPFQuiKiXeHnjaMQ"}}},"title":"ollama/README.md at main · jmorganca/ollama"}